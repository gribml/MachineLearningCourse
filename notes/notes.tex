\documentclass[a4paper,10pt]{article}
%\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\titleformat{\chapter}[hang]{\bf\Huge}{\thechapter}{1cm}{}

\pagestyle{plain}
% -------------------- this stuff for code --------------------

\usepackage{anysize}
\marginsize{30mm}{30mm}{20mm}{20mm}

\newenvironment{formal}{%
  \def\FrameCommand{%
    \hspace{1pt}%
    {\color{blue}\vrule width 2pt}%
    {\color{formalshade}\vrule width 4pt}%
    \colorbox{formalshade}%
  }%
  \MakeFramed{\advance\hsize-\width\FrameRestore}%
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}%
  \vspace{2pt}\vspace{2pt}%
}
{%
  \vspace{2pt}\end{adjustwidth}\endMakeFramed%
}

\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}

\usepackage{color}
\usepackage{dsfont}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled]{helvet}
\usepackage{inconsolata}


\definecolor{colKeys}{rgb}{0,0,0.9} 
\definecolor{colIdentifier}{rgb}{0,0,0} 
\definecolor{colString}{rgb}{0.7,0,0} 
\definecolor{colComments}{rgb}{0,0.6,0} 
\usepackage{listings}
\lstset{
  language=python,
  stringstyle=\color{colString},
  keywordstyle=\color{colKeys},
  identifierstyle=\color{colIdentifier},
  commentstyle=\color{colComments},
  numbers=left,
  tabsize=4,
  frame=single,
  breaklines=true,
  basicstyle=\small\ttfamily,
  numberstyle=\tiny\ttfamily,
  framexleftmargin=0mm,
  xleftmargin=7mm,
  xrightmargin=7mm,
  frameround={tttt},
  captionpos=b
}

%% Headers and footers
\usepackage[section]{placeins}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{plain}\cleardoublepage}}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage[T1]{fontenc}
\usepackage{enumerate}
\usepackage{afterpage,lastpage}
\usepackage[includeheadfoot,margin=2.5cm]{geometry}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 

% -------------------- end of code stuff --------------------



\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\makeatletter \def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill
\kern \z@}


\author{Paul Gribelyuk (pg1312, a5)}
\makeatother
\title{\Large \#DOC395 - Study Notes for Machine Learning}
\date{\today}

\begin{document}
\maketitle
\section{Concept Learning}
Idea is to find a function $h$ which takes input data (a tuple of states) and maps to a discrete set of classes, i.e. classifies it.  Concept learning is \emph{supervised}, \emph{eager} learning.  We have a natural ordering on the set of hypotheses based on how specific / general they are.  THe most general is <?, ?, ?, ?, ?, ?> while the most specific is <a, b, c, d, e, f> 
\begin{itemize}
\item \emph{Find-S} algorithm starts with the most specific hypothesis, i.e. that no part of the input can predict the output.  It then iterates over the positive-target data, modifying the component of the working hypothesis which would make it satisfy the example.  Solution is not unique.
\item \emph{Candidate-Elimination} algorithm starts with most general hypotheses as well as most specific ones.  The version space $VS$ is the set of all $h\in H$ consistent with the data.  There are two boundaries: on the specific side ($S$) and on the general side ($G$).  Initially, $G = (?,?,?,?,\ldots)$ and $S=(0,0,0,\ldots)$.  The algorithms progresses:
\begin{itemize}
\item If an example is a positive example, then we attempt to generalize the specific set $S$ as little as possible to be consistent with it.  However, first it is important to make sure that $G$ is also in agreement with this data point.
\item If an example is negative, make G more specific by getting rid of inconsistent hypotheses in $G$ and replacing them with those remain more general than $S$ but are consistent with it.  First, make sure that $S$ is consistent with this data point.
\end{itemize}
\end{itemize}

\section{Decision Trees}
Entropy of a set $S$ is defined:
$$
E(S) = -\sum_{i\in C} p_i\log_2(p_i)
$$
where $C$ is all the classes values in $S$ can take and $p_i$ is probability (occurence) of class $i$.
Information gain (IG) for an attribute $A$ relative to a set $S$ is defined
$$
IG(S, A) = E(S) - \sum_{v\in values(A)}\frac{|S_v|}{|S|}E(S_v)
$$
where $S_v$ is the subset of $S$ having value $v$ for attribute A.  There is an alternative measure by which to split the data, defined as the \emph{split information}
$$
SI(S, A) = -\sum_{i=1}^c \frac{|S_i|}{|S|}\log_2\left(\frac{|S_i|}{|S|}\right)
$$
where $S_i$ is result of partioning $S$ across values of the attribute A.  This is essentially an entropy but with respect to values of A rather than with respect to target values.  Then the \emph{gain ratio} is defined as:
$$
GR(S, A) = \frac{IG(S, A)}{SI(S, A)}
$$
This discourages attributes with many values (unif distributed).
\section{Cross-Validation}
Take dataset of $m = |D|$ items and split into $N$ `folds'.  Then for each fold $F_i$, train on dataset $D - \{F_i\}$ and test the learned model on the dataset $F_i$.  Average over all folds to obtain unbiased error estimate.

\section{Neural Networks}
\subsection{Perceptron}
Trying to separate positive and negative examples according to a hyperplane which splits the example space so as to minimize the number of example incorrectly classified by the hyperplane (perceptron).  This is the simplest case of a neural network as $n$ inputs from the $n$-dimensional input vector go into a single neuron, which produces an output according to a rule:
$$
h(x_1, x_2, \ldots, x_n) = sgn\left(\sum_{i=0}^N w_i x_i\right) = sgn(\mathbf{w}\cdot\mathbf{x})
$$
where $x_0$ is the \emph{bias}, usually $1$.  The rules to learn the weights $w_i$ are \emph{perceptron training} and \emph{gradient descent}.
\begin{itemize}
\item Perceptron training rule goes through each example that is misclassified by the previous weights and updates them so the number of misclassified examples decreases... this works when the examples are perfectly linearly separable:
$$
w_i \leftarrow w_i + \eta (t - sgn(\mathbf{w}\cdot\mathbf{x})) x_i
$$
where $t$ is the known target classification of $x_i$.
\item Gradient descent considers the unthresholded perceptron:
$$
h(\mathbf{x}) = \mathbf{w}\cdot\mathbf{x}
$$
and minimizes the error function:
$$
E(\mathbf{w}) = \frac{1}{2}\sum_{d\in D} (t_d - \mathbf{w}\cdot\mathbf{x_d})^2
$$
The weight update rule is:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla E(\mathbf{w}) = \eta \sum_{d\in D} (t_d - \mathbf{w}\cdot\mathbf{x_d})x_{i,d}
$$
where $x_{i,d}$ is the $i-$th component of the $d-$th input data.
\item Stochastic gradient descent updates the weights without summing over all data points, but rather after each data point.  This usually needs a smaller learning rate $\eta$.  So:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla E(\mathbf{w}) = \eta (t_d - \mathbf{w}\cdot\mathbf{x_d})x_{i,d}
$$
\end{itemize}
\subsection{Multilayer Neural Networks}
Input layer is the set of $\{x_1, x_2, \ldots, x_n\}$.  This goes into neuron $j$ in the $k$-th hidden layer as:
$$
s_j^{(k)} = \sum_{i=0}^n w_{ij}^{k} x_i^{(k-1)}
$$
and the output from neuron $j$ is:
$$
x_j^{(k)} = a\left( s_j^{(k)}\right) = a\left(\sum_{i=0}^n w_{ij}^{k} x_i^{(k-1)}\right)
$$
where $a()$ is the activation function which can be sigmoid:
$$
a(x) = \sigma(x) = \frac{1}{1 + e^{-x}}
$$
or the $tanh$:
$$
a(x) = \tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{1- e^{-2x}}{1 + e^{-2x}}
$$
Derivative properties: $\sigma'(x) = \sigma(x)(1 - \sigma(x))$ and $\tanh'(x) = (1 - \tanh^2(x))$  To learn the weights $w_{ij}^{(l)}$, we can do gradient descent again but computing $\frac{\partial E(\mathbf{w})}{w_{ij}^{(l)}}$ could be expensive.  \emph{Backpropagation} has two steps:
\begin{itemize}
\item Propagate input forward using weights $w_{ij}^{(l)}$ already in place and calculating inputs $s_i^{(l)}$ and outputs $x_i^{(l)}$ at each neuron.  
\item Calculate errors at the output:
$$
\delta_k^{(L)} \leftarrow (a(s_k^{(L)}) - y_k) a'(s_k^{(L)})
$$
when using the standard square-error function.  Then the previous deltas are:
$$
\delta_k^{(l-1)} = a'(s_i^{(l-1)}) \sum_j \delta_j^{(l)} w_{kj}^{(l)}
$$
Once these are computed, the weights are updated:
$$
w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} + \eta\delta_j^{(l)}x_{i}^{(l-1)}
$$
\end{itemize}
We can add momentum to the weight update by considering the previous weight update as well:
$$
\Delta w_ij^{(l)}(n) = \eta \delta_j^{(l)} x_i^{(l-1)} + \alpha\Delta w_{ij}^{(l)}(n-1)
$$
where $n$ is the update number and $\alpha$ is the weight given to the prior weight update.  We can add a penalty to high weights by the following error function:
$$
E(\mathbf{w}) = \frac{1}{2}\sum_{d\in D}\sum_{k} (t_{k,d} - x_k^{(L)})^2 + \gamma\sum_{i,j,l}w_{i,j}^{l}
$$
So, for the errors at the output layer with sigmoid activation:
$$
\delta_j^{(L)} = 
$$
\section{Case Based Learning}
Training is \emph{lazy}, so decisions about target function are put off until needed.  Hypothesis is continuously being updated as new data arrives.  Each new datum has real or discrete attributes $x = (a_1(x), a_2(x), \ldots, a_n(x))$.  Then we have a distance function:
$$
d(x_i, x_j) = \sqrt{\sum_{p=1}^n (a_p(x) - a_p(y))^2}
$$
The algorithm trains by updating the set of examples (cases).  KNN predicts target function value of $x_q$ by retrieving the set of examples $x_1, x_2, \ldots x_k$ closest in distance to $x_q$.  Then the discrete-valued target function evaluation is:
$$
\hat{f}(x_q) \leftarrow \argmax_{v}\sum_{i=1}^k \delta(v, f(x_i))
$$
is the most frequently occuring target value of the $k$ nearest neigbors.  For a continuous target function:
$$
\hat{f}(x_q) \leftarrow \argmax_v \frac{\sum_{i=1}^k f(x_i)}{k}
$$
is the average of the target values of the $k$ nearest neighbors.  We can also apply weights to each of the $k$ neighbors:
\begin{eqnarray}
\hat{f}(x_q) &\leftarrow& \argmax_v \sum_{i=1}^k w\delta(v, f(x_i)) \\
w_i &=& \frac{1}{d(x_q, x_i)^2}
\end{eqnarray}
We can also do locally-weighted regression to calculate the target value of an input.  This involves defining an error function:
$$
E(x_q) = \frac{1}{2}\sum_{x\in k\text{ }neighbors} (f(x) - \hat{f}(x))^2 K(d(x_q, x))
$$
and defining the local target function as a linear regression, having the weights $w_i$ which minimize $E(x_q)$
$$
\hat{f}(x_q) = w_0 + w_1 a_1(x_q) + \ldots + w_n a_n(x_q)
$$
We minimize the error function using gradient descent:
$$
w_{s+1} \leftarrow w_s + \eta \sum_{x\in k\text{ }neighbors} a_s(x)(f(x)-\hat{f}(x)) K(d(x_q, x))
$$
Case based reasoning is a marginal extension of KNN to different metrics and non-numeric classification of instances.  There are issues with storing/organizing cases:
\begin{itemize}
\item Flat: Retrieval on a case-by-case search
\item Clustered: Store cases into clusters based on similarity.  Retrieval is quicker because just have to look through cluster for similar case
\item Hierarchical: cases with similar features grouped together and features are linked to categories, which are used to retrive cases.
\end{itemize}

\section{Genetic Algorithms}
A method to search set of all hypotheses to maximize a pre-defined \emph{fitness} function.  The algorithm evalutes all hypotheses in a population, moves some of the fittest ones (probabilistically) to the next round and mutates/cross-over some of them.  Probability of selecting a hypothesis to be included in the next round is:
$$
p(h_i) = \frac{F(h_i)}{\sum_j F(h_j)}
$$
where $F(\cdot)$ is the fitness function and sum is taken over whole population.  Alternatively, it's possible to use \emph{tournament selection} (choose two random hypotheses and the one with higher fitness moves on) or \emph{rank selection} (rank hypotheses by fitness then $p(h_i)\propto i$ where $i$ is the rank of the hypothesis).  For cross-over:
\begin{itemize}
\item Single-point: select a single point along the sequence-representation of two hypotheses, and cut them at that point.  Then switch the equal-length parts between them and re-attach to form new hypotheses.
\item Two-point: same principle as single-point, but with two places to cut the hypothesis and exchange between the two ``parent'' hypotheses.
\item Uniform: Generate a random crossover mask and apply to both parents to exchange ``genes'' and create new hypotheses.
\end{itemize}
For \emph{mutation} pick a random point in a hypothesis to flip the bit.  GA is an example of \emph{beam search} which is an optimization of \emph{best-first search} which explores a tree along the branch showing the highest heuristic measure $h(\cdot)$.  This means it evaluates all branches at each level to determine the best one to take.  Beam search only evaluates $k<n$.

\section{Evaluating Hypotheses}
Classification error rate:
$$
e = \frac{FP + FN}{TP + TN + FP + FN}
$$
Recall and Precision:
$$
R = \frac{TP}{TP + FN} \qquad P = \frac{TP}{TP + FP}
$$
A better measure of how well the classifier does:
$$
F_{\alpha} = (1 + \alpha)\frac{P\cdot R}{\alpha \cdot P + R}
$$
Estimating error in a confidence interval $N\%$:
$$
error_D(h) = error_S(h) \pm z_N\sqrt{\frac{error_S(h)(1-error_S(h))}{n}}
$$
\emph{t-test}
$$
t = \frac{\bar{x_C} - \bar{x_T}}{SE(\bar{x_C} - \bar{x_T})}
$$
where the denominator is defined as:
$$
SE(\bar{x_C} - \bar{x_T}) = \sqrt{\frac{var_T}{n_T} + \frac{var_C}{n_C}}
$$












\end{document}
